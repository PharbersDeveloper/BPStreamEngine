#spark.yarn.jars=hdfs://161.189.48.242:8020/jars/sparkJars
#spark.yarn.archive=hdfs://161.189.48.242:8020/jars/sparkJars
spark.driver.memory=2g
spark.driver.cores=2
spark.scheduler.mode=FAIR
spark.executor.memory=4g
spark.executor.cores=4
spark.executor.instances=4
spark.sql.streaming.numRecentProgressUpdates=1000
spark.sql.shuffle.partitions=200
#spark.extraListeners=com.pharbers.StreamEngine.Utils.Strategy.Session.Spark.BPSparkListener
#es config
es.nodes=59.110.31.215
es.nodes.wan.only=true
es.index.auto.create=true
#spark.driver.extraJavaOptions=-Dlog4j.configuration=file:D:\\code\\pharbers\\BPStream\\BPStreamEngine\\logs\\log4j.properties
#spark.executor.extraJavaOptions=-Dlog4j.configuration=src/main/resources/log4j2.xml
#spark.driver.host=192.168.100.116
spark.sql.hive.convertMetastoreParquet=false
spark.hadoop.parquet.block.size=67108864
#spark.yarn.jars=hdfs://starLord:8020/jars/sparkJars
#spark.yarn.archive=hdfs://starLord:8020/jars/sparkJars
#spark.driver.memory=6g
#spark.driver.cores=4
#spark.scheduler.mode=FAIR
#spark.executor.memory=6g
#spark.executor.cores=4
#spark.executor.instances=4
#spark.sql.streaming.numRecentProgressUpdates=100
#spark.sql.streaming.minBatchesToRetain=10
#spark.ui.retainedStages=50
#spark.ui.retainedJobs=50
#spark.ui.retainedTasks=500
#spark.sql.shuffle.partitions=20
##spark.extraListeners=com.pharbers.StreamEngine.Utils.Session.Spark.BPSparkListener
##es config
#es.nodes=59.110.31.215
#es.nodes.wan.only=true
#es.index.auto.create=true
##spark.driver.extraJavaOptions=-Dlog4j.configuration=file:D:\\code\\pharbers\\BPStream\\BPStreamEngine\\logs\\log4j.properties
##spark.executor.extraJavaOptions=-Dlog4j.configuration=src/main/resources/log4j2.xml
#spark.sql.hive.convertMetastoreParquet=false
#spark.hadoop.parquet.block.size=67108864