#spark.yarn.jars=hdfs://161.189.48.242:8020/jars/sparkJars
#spark.yarn.archive=hdfs://161.189.48.242:8020/jars/sparkJars
spark.driver.memory=4g
spark.driver.cores=4
spark.scheduler.mode=FAIR
spark.executor.memory=4g
spark.executor.cores=2
spark.executor.instances=5
spark.sql.streaming.numRecentProgressUpdates=1000
spark.sql.shuffle.partitions=200
#spark一个分区打开文件的最大字节数，针对parquet调整为了10m
spark.sql.files.maxPartitionBytes=10485760
#spark合并小文件的阈值，最大合并为spark.sql.files.maxPartitionBytes，针对parquet调整为了1m
spark.sql.files.openCostInBytes=1048576
#spark批处理的listener
#spark.extraListeners=com.pharbers.StreamEngine.Utils.Strategy.Session.Spark.BPSparkListener
#es config
es.nodes=59.110.31.215
es.nodes.wan.only=true
es.index.auto.create=true
#对于spark log4j的配置。这儿其实没发挥作用，需要在submit中配置，或者通过ide运行时添加java运行参数
#spark.driver.extraJavaOptions=-Dlog4j.configuration=file:D:\\code\\pharbers\\BPStream\\BPStreamEngine\\logs\\log4j.properties
#spark.executor.extraJavaOptions=-Dlog4j.configuration=src/main/resources/log4j2.xml
#spark.driver.host=192.168.100.116
spark.sql.hive.convertMetastoreParquet=false
spark.hadoop.parquet.block.size=67108864